{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import logistic\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9415\n",
      "[(array([-0.99255664, -0.09850755,  1.28068487, -0.21883622, -0.68517474,\n",
      "        1.47696556, -0.69625027,  0.20211889, -0.40431657,  0.39336439,\n",
      "       -0.44355195,  2.05617479, -0.54803995,  0.84628832, -0.73711619,\n",
      "        1.98127596,  0.41923591,  0.893298  ,  0.56276857, -0.22347953,\n",
      "       -0.17147022,  1.62890066,  1.01785961, -2.20311793,  0.73077756,\n",
      "        0.43549215,  0.52516738,  0.72431454,  0.14734245,  0.17624503,\n",
      "       -0.01190663, -1.34815375, -1.47993372, -0.62495667, -1.75821009,\n",
      "        1.46175899,  0.9440697 ,  0.30296945,  0.11635087, -1.50477573,\n",
      "        0.60928502, -0.36456007,  0.75549871,  0.04874346, -0.66939099,\n",
      "       -0.15267852, -0.79950737,  0.73294968, -1.71637004, -0.64090541,\n",
      "       -1.74597354,  0.30217295, -1.42545454,  0.11945793, -0.10902602,\n",
      "        0.82596859,  0.03332325,  0.99495543,  1.98529061,  2.47761857,\n",
      "       -0.99467733, -1.35298519, -1.4023889 , -0.25696657, -1.3336469 ,\n",
      "        0.34520775,  1.17876415, -0.84555489,  0.12349458,  0.10502958,\n",
      "        0.1381491 , -1.66148736, -0.49060772,  0.79343421, -0.19148387,\n",
      "       -0.87383535, -1.21974904, -1.11395703,  1.621742  , -0.11752459,\n",
      "       -0.49616656,  0.37207804,  1.13624315, -0.07292182, -0.03283003,\n",
      "       -0.9425125 , -1.34506955,  0.53278998,  1.1622445 ,  0.95925585,\n",
      "       -0.02781297, -0.46422112, -0.43436149, -0.51543668, -0.14322266,\n",
      "        0.52758236,  0.91671706, -0.90366672, -1.80181946,  1.21581707]), 0), (array([-1.56160258, -0.43529784, -0.10867894,  0.42888287,  0.66821327,\n",
      "        0.38785023, -0.20984953,  1.12331523,  1.58763014, -0.79443644,\n",
      "       -1.49707916, -1.70661596, -0.05588795,  0.78605802,  0.98171053,\n",
      "        0.32425937, -1.64298152, -1.07484406,  1.09582019, -0.94050839,\n",
      "        0.23537584,  0.4004829 ,  0.59227723,  2.190041  ,  2.5434236 ,\n",
      "       -0.94709945, -0.61743859, -0.43977827, -0.60898451, -1.66423059,\n",
      "       -1.52872197,  0.63537669, -1.88028409, -2.0299327 ,  0.57779606,\n",
      "        0.20520137, -1.45734371,  0.50458245, -0.9615754 , -0.50812433,\n",
      "       -0.76856378, -0.45208434, -0.24566597,  0.24285396, -1.00842023,\n",
      "       -0.46388758,  0.7439337 ,  1.99376205,  0.35985207,  0.69086627,\n",
      "       -2.38921465, -0.089651  , -2.03144247,  0.53903731,  1.53293271,\n",
      "       -1.11916376,  0.67108743, -0.84325959,  0.98767734,  0.49841034,\n",
      "       -0.57788345,  0.91273659,  1.07239786, -0.51361655,  0.82295884,\n",
      "       -1.0144446 , -1.21724314, -0.67406003, -0.34305612,  1.61163344,\n",
      "        0.15361699, -1.09868749, -0.22471476, -1.18594198, -0.50581462,\n",
      "       -0.04201267,  0.42920695, -0.28160815,  0.37666794, -0.278934  ,\n",
      "        1.426666  , -0.3609695 , -0.52749255,  0.37530543,  1.76987555,\n",
      "        0.16642789, -0.41640492, -0.29434622, -0.70079456, -0.52468491,\n",
      "        0.7125388 ,  0.17049434, -0.92497279,  0.23252593,  0.36931701,\n",
      "       -0.85954763, -0.79498851, -1.2042422 , -2.28972071,  1.20609717]), 1)]\n",
      "beta: [0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 0. 0.\n",
      " 0. 0. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "n_train = 2000\n",
    "n_eval = 100\n",
    "n_test = 100\n",
    "p = 100#00\n",
    "sparse_p = 10\n",
    "\n",
    "sparsity_index = [ _ < sparse_p for _ in range(p)]\n",
    "random.shuffle(sparsity_index)\n",
    "\n",
    "beta = np.zeros(p)\n",
    "for _ in range(p):\n",
    "    if sparsity_index[_]:\n",
    "        beta[_] = 2\n",
    "X_train = [np.random.normal(size = p) for _ in range(n_train)]\n",
    "X_eval = [np.random.normal(size = p) for _ in range(n_eval)]\n",
    "X_test = [np.random.normal(size = p) for _ in range(n_test)]\n",
    "train_dataset = []\n",
    "eval_dataset = []\n",
    "test_dataset = []\n",
    "noise_ratio = []\n",
    "for _ in X_train:\n",
    "    #y = np.random.binomial(n = 1, p = logistic.cdf(np.dot(_, beta)))\n",
    "    y = int(logistic.cdf(np.dot(_, beta) + np.random.normal(scale = 1)) > 0.5)\n",
    "    noise_ratio.append((np.dot(_, beta) + np.random.normal(scale = 1))*np.dot(_, beta) >0 )\n",
    "    train_dataset.append((_,y))\n",
    "print(np.mean(noise_ratio))\n",
    "for _ in X_eval:\n",
    "    #y = np.random.binomial(n = 1, p = logistic.cdf(np.dot(_, beta)))\n",
    "    y = int(logistic.cdf(np.dot(_, beta)+ np.random.normal(scale = 1)) > 0.5)\n",
    "    eval_dataset.append((_,y))\n",
    "for _ in X_test:\n",
    "    #y = np.random.binomial(n = 1, p = logistic.cdf(np.dot(_, beta)))\n",
    "    y = int(logistic.cdf(np.dot(_, beta)+ np.random.normal(scale = 1)) > 0.5)\n",
    "    test_dataset.append((_,y))\n",
    "print(train_dataset[:3][:2])\n",
    "print(f\"beta: {beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, ind):\n",
    "        return self.data[ind]\n",
    "\n",
    "train_set = TrainDataset(train_dataset)\n",
    "test_set  = TrainDataset(test_dataset)\n",
    "eval_set  = TrainDataset(eval_dataset)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=n_eval, shuffle=False)\n",
    "eval_loader  = DataLoader(eval_set,  batch_size=n_test, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=25, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=25, out_features=1, bias=False)\n",
      "  )\n",
      ")\n",
      "('Z', single_layer(\n",
      "  (mlp): Linear(in_features=2525, out_features=1, bias=False)\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(p, 25 ,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 1,bias=False)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "\n",
    "model = MLP().to(device)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#torch.optim.lr_scheduler()\n",
    "\n",
    "##\n",
    "print(model)\n",
    "\n",
    "class single_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(single_layer, self).__init__()\n",
    "        self.mlp = nn.Linear( (p + 0) * 25  + (25 + 0) * 1, 1, bias= False)\n",
    "    def forward(self, x):\n",
    "        out = self.mlp(x)\n",
    "        return out\n",
    "\n",
    "model = MLP().to(device)\n",
    "var_Z = single_layer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer2 = torch.optim.SGD(var_Z.parameters(), lr=1e-1)\n",
    "print((\"Z\", var_Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mlp.0.weight', Parameter containing:\n",
      "tensor([[-0.0650, -0.0010, -0.0521,  ...,  0.0245,  0.0350,  0.0087],\n",
      "        [-0.0716,  0.0811, -0.0584,  ...,  0.0306,  0.0548,  0.0138],\n",
      "        [-0.0274, -0.0257, -0.0638,  ...,  0.0615, -0.0323,  0.0153],\n",
      "        ...,\n",
      "        [ 0.0328,  0.0557,  0.0139,  ..., -0.0046, -0.0082,  0.0477],\n",
      "        [ 0.0883, -0.0404, -0.0180,  ...,  0.0324,  0.0954,  0.0755],\n",
      "        [-0.0217,  0.0082, -0.0038,  ..., -0.0872,  0.0482, -0.0065]],\n",
      "       requires_grad=True))\n",
      "('mlp.2.weight', Parameter containing:\n",
      "tensor([[-0.0493, -0.0503,  0.0279,  0.0233, -0.1056, -0.0151,  0.1458, -0.1005,\n",
      "         -0.0736, -0.0118, -0.0963,  0.1630,  0.1574,  0.1516, -0.0310,  0.0708,\n",
      "          0.1207, -0.1783,  0.0906, -0.1006,  0.1613,  0.1243, -0.0337, -0.0112,\n",
      "         -0.1828]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    #if  name.endswith(\"weight\"):\n",
    "        print((name,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0650, -0.0010, -0.0521,  ..., -0.0337, -0.0112, -0.1828],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer2.zero_grad()\n",
    "#print([_ for _ in model.parameters()])\n",
    "torch.cat([param.view(-1)  for param in model.parameters()])\n",
    "#[torch.cat([param[0].view(-1), param[1].view(-1)])  for param in model.parameters()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('grad', tensor([[-0.3978, -0.0231, -0.4867,  ..., -1.5970, -0.7254, -1.0626]]))\n",
      "('mean_value', tensor(0.9900, grad_fn=<MeanBackward0>))\n",
      "weight:tensor([-4.0798e-02, -5.0233e-05, -4.8865e-02,  ..., -1.6177e-01,\n",
      "         7.4795e-02,  1.0718e-01], grad_fn=<CatBackward0>)\n",
      "weight:tensor([-4.1120e-02,  1.2928e-05, -4.7404e-02,  ..., -1.6373e-01,\n",
      "         7.5836e-02,  1.0798e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[0.0100, 0.0099, 0.0100,  ..., 0.0100, 0.0100, 0.0100]]))\n",
      "('mean_value', tensor(0.9994, grad_fn=<MeanBackward0>))\n",
      "weight:tensor([-3.3763e-02, -3.1542e-06, -3.9978e-02,  ..., -1.6433e-01,\n",
      "         7.6908e-02,  1.0809e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 3 | Batch 62 | train Loss  26.92, loss1   0.72, loss2  25.21, loss3   0.99, loss4   0.99\n",
      " non-zero weights 2526 | val accuracy  43.75 \n",
      "weight:tensor([-2.6469e-02, -3.3988e-07, -3.2543e-02,  ..., -1.6480e-01,\n",
      "         7.7754e-02,  1.0857e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[-0.2535,  0.0100, -0.3143,  ..., -1.6378, -0.7676, -1.0756]]))\n",
      "('mean_value', tensor(0.9984, grad_fn=<MeanBackward0>))\n",
      "Epoch 4 | Batch 62 | train Loss  25.91, loss1   0.68, loss2  25.23, loss3   0.00, loss4   0.97\n",
      " non-zero weights 2526 | val accuracy  50.00 \n",
      "weight:tensor([-2.5499e-02,  1.8847e-05, -3.0752e-02,  ..., -1.6676e-01,\n",
      "         8.0335e-02,  1.1070e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 5 | Batch 62 | train Loss  25.86, loss1   0.63, loss2  25.23, loss3   0.00, loss4   0.97\n",
      " non-zero weights 2526 | val accuracy  75.00 \n",
      "weight:tensor([-2.5249e-02, -2.4493e-05, -3.0193e-02,  ..., -1.7033e-01,\n",
      "         8.3649e-02,  1.1339e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[0.0100, 0.0098, 0.0100,  ..., 0.0100, 0.0100, 0.0100]]))\n",
      "('mean_value', tensor(0.9991, grad_fn=<MeanBackward0>))\n",
      "Epoch 6 | Batch 62 | train Loss  26.70, loss1   0.64, loss2  25.20, loss3   0.86, loss4   0.86\n",
      " non-zero weights 2526 | val accuracy  81.25 \n",
      "weight:tensor([-1.9003e-02,  9.9057e-06, -2.3733e-02,  ..., -1.7234e-01,\n",
      "         8.5764e-02,  1.1555e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 7 | Batch 62 | train Loss  26.61, loss1   0.67, loss2  25.20, loss3   0.74, loss4   0.74\n",
      " non-zero weights 2526 | val accuracy  62.50 \n",
      "weight:tensor([-1.1560e-02,  4.0907e-05, -1.6364e-02,  ..., -1.7435e-01,\n",
      "         8.7996e-02,  1.1700e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[-0.1047,  0.0097, -0.1524,  ..., -1.7337, -0.8702, -1.1606]]))\n",
      "('mean_value', tensor(0.9981, grad_fn=<MeanBackward0>))\n",
      "Epoch 8 | Batch 62 | train Loss  25.87, loss1   0.65, loss2  25.21, loss3   0.00, loss4   0.72\n",
      " non-zero weights 2526 | val accuracy  62.50 \n",
      "weight:tensor([-1.0593e-02,  4.3433e-05, -1.4766e-02,  ..., -1.7815e-01,\n",
      "         9.1973e-02,  1.2113e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 9 | Batch 62 | train Loss  25.85, loss1   0.64, loss2  25.21, loss3   0.00, loss4   0.73\n",
      " non-zero weights 2526 | val accuracy  68.75 \n",
      "weight:tensor([-1.0295e-02,  1.8639e-05, -1.4424e-02,  ..., -1.8296e-01,\n",
      "         9.5990e-02,  1.2534e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[0.0100, 0.0097, 0.0100,  ..., 0.0100, 0.0100, 0.0100]]))\n",
      "('mean_value', tensor(0.9986, grad_fn=<MeanBackward0>))\n",
      "Epoch 10 | Batch 62 | train Loss  26.45, loss1   0.63, loss2  25.19, loss3   0.63, loss4   0.64\n",
      " non-zero weights 2526 | val accuracy  75.00 \n",
      "weight:tensor([-4.5096e-03, -3.6739e-05, -8.2718e-03,  ..., -1.8627e-01,\n",
      "         9.9492e-02,  1.2870e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 11 | Batch 62 | train Loss  26.35, loss1   0.63, loss2  25.19, loss3   0.54, loss4   0.54\n",
      " non-zero weights 2526 | val accuracy  87.50 \n",
      "weight:tensor([ 3.8153e-05, -4.7210e-05, -1.1816e-03,  ..., -1.8972e-01,\n",
      "         1.0237e-01,  1.3207e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[ 9.4745e-03,  9.6207e-03, -6.9896e-04,  ..., -1.8880e+00,\n",
      "         -1.0140e+00, -1.3110e+00]]))\n",
      "('mean_value', tensor(0.9976, grad_fn=<MeanBackward0>))\n",
      "Epoch 12 | Batch 62 | train Loss  25.87, loss1   0.67, loss2  25.19, loss3   0.00, loss4   0.53\n",
      " non-zero weights 2526 | val accuracy  56.25 \n",
      "weight:tensor([ 2.3435e-05, -2.9405e-05,  3.6751e-05,  ..., -1.9535e-01,\n",
      "         1.0698e-01,  1.3674e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 13 | Batch 62 | train Loss  25.81, loss1   0.61, loss2  25.19, loss3   0.00, loss4   0.53\n",
      " non-zero weights 2526 | val accuracy  81.25 \n",
      "weight:tensor([ 6.4761e-06,  1.3284e-05, -5.1084e-06,  ..., -2.0140e-01,\n",
      "         1.1224e-01,  1.4250e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[0.0099, 0.0099, 0.0100,  ..., 0.0100, 0.0100, 0.0100]]))\n",
      "('mean_value', tensor(0.9978, grad_fn=<MeanBackward0>))\n",
      "Epoch 14 | Batch 62 | train Loss  26.27, loss1   0.63, loss2  25.17, loss3   0.46, loss4   0.46\n",
      " non-zero weights 2526 | val accuracy  62.50 \n",
      "weight:tensor([ 3.8279e-05, -9.2782e-06,  2.1054e-05,  ..., -2.0604e-01,\n",
      "         1.1627e-01,  1.4691e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 15 | Batch 62 | train Loss  26.15, loss1   0.60, loss2  25.17, loss3   0.39, loss4   0.39\n",
      " non-zero weights 2526 | val accuracy  87.50 \n",
      "weight:tensor([-6.7835e-05,  6.0765e-06,  4.5569e-05,  ..., -2.1082e-01,\n",
      "         1.1968e-01,  1.5086e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[ 0.0095,  0.0099,  0.0092,  ..., -2.0991, -1.1873, -1.4992]]))\n",
      "('mean_value', tensor(0.9968, grad_fn=<MeanBackward0>))\n",
      "Epoch 16 | Batch 62 | train Loss  25.77, loss1   0.60, loss2  25.17, loss3   0.00, loss4   0.38\n",
      " non-zero weights 2526 | val accuracy  93.75 \n",
      "weight:tensor([ 2.1926e-05,  4.4201e-06, -2.0780e-05,  ..., -2.1755e-01,\n",
      "         1.2495e-01,  1.5672e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 17 | Batch 62 | train Loss  25.77, loss1   0.59, loss2  25.17, loss3   0.00, loss4   0.39\n",
      " non-zero weights 2526 | val accuracy  93.75 \n",
      "weight:tensor([-2.4778e-07, -3.7852e-05, -6.2825e-05,  ..., -2.2439e-01,\n",
      "         1.3093e-01,  1.6331e-01], grad_fn=<CatBackward0>)\n",
      "('grad', tensor([[0.0098, 0.0096, 0.0097,  ..., 0.0100, 0.0100, 0.0100]]))\n",
      "('mean_value', tensor(0.9968, grad_fn=<MeanBackward0>))\n",
      "Epoch 18 | Batch 62 | train Loss  26.12, loss1   0.64, loss2  25.14, loss3   0.34, loss4   0.34\n",
      " non-zero weights 2526 | val accuracy  68.75 \n",
      "weight:tensor([-1.2323e-06, -2.1299e-06,  7.1384e-05,  ..., -2.3016e-01,\n",
      "         1.3507e-01,  1.6801e-01], grad_fn=<CatBackward0>)\n",
      "Epoch 19 | Batch 62 | train Loss  25.99, loss1   0.56, loss2  25.14, loss3   0.29, loss4   0.29\n",
      " non-zero weights 2526 | val accuracy  93.75 \n",
      "weight:tensor([ 6.4559e-06, -4.8158e-06, -5.7872e-05,  ..., -2.3564e-01,\n",
      "         1.3899e-01,  1.7248e-01], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "epochs = 20#20\n",
    "lambda_2 = 0.01\n",
    "lambda_3 = 10\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model = MLP().to(device)\n",
    "var_Z = single_layer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer2 = torch.optim.SGD(var_Z.parameters(), lr=1e-1)\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n",
    "def projection_on_z(z_network):\n",
    "    with torch.no_grad():\n",
    "        for param in z_network.parameters():\n",
    "            param.data.clamp_(min = 0, max = 1)\n",
    "\n",
    "\n",
    "for param in var_Z.parameters():\n",
    "    #param.data.clamp_(min = 1 - 0.1/lambda_3, max = 1 - 0.1/lambda_3)\n",
    "    param.data.clamp_(min = 0.5, max = 0.5)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    if epoch % 2 == 0:\n",
    "        projection_on_z(var_Z)\n",
    "        optimizer2.zero_grad()\n",
    "        vec_z = torch.cat([param.view(-1)  for param in var_Z.parameters()]) \n",
    "        vec_weight = torch.cat([param.view(-1)  for param in model.parameters()])\n",
    "        #vec_z = #torch.tensor([ lambda_2 /lambda_3 < torch.abs(vec_weight[_]) for _ in range()])\n",
    "        #vec_z = lambda_2 /lambda_3 * torch.ones_like(vec_weight) < torch.abs(vec_weight)\n",
    "        #vec_z = vec_z.float()\n",
    "        loss2 = lambda_2 * torch.sum(vec_z)\n",
    "        loss3 = lambda_3 * torch.norm((torch.ones_like(vec_z) - vec_z) * vec_weight, 1)\n",
    "        loss4 = 0.01 * torch.norm(vec_weight, 1)\n",
    "        loss = loss2 + loss3\n",
    "        loss.backward()\n",
    "        print((\"grad\",var_Z.mlp.weight.grad))\n",
    "        #print((\"z value\",var_Z.mlp.weight ))\n",
    "        print((\"mean_value\",torch.mean(var_Z.mlp.weight)))\n",
    "        optimizer2.step()\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "        projection_on_z(var_Z)\n",
    "        optimizer.zero_grad()\n",
    "        #optimizer2.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "\n",
    "        output = model(x).squeeze() \n",
    "        \n",
    "        loss1 = criterion(output, y)\n",
    "        vec_z = torch.cat([param.view(-1)  for param in var_Z.parameters()]) \n",
    "        vec_weight = torch.cat([param.view(-1)  for param in model.parameters()])\n",
    "        #vec_z = #torch.tensor([ lambda_2 /lambda_3 < torch.abs(vec_weight[_]) for _ in range()])\n",
    "        #vec_z = lambda_2 /lambda_3 * torch.ones_like(vec_weight) < torch.abs(vec_weight)\n",
    "        #vec_z = vec_z.float()\n",
    "        loss2 = lambda_2 * torch.sum(vec_z)\n",
    "        loss3 = lambda_3 * torch.norm((torch.ones_like(vec_z) - vec_z) * vec_weight, 1)\n",
    "        loss4 = 0.01 * torch.norm(vec_weight, 1)\n",
    "        loss = loss1.mean() + loss2 + loss3\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        #print(('z', torch.mean(vec_z), vec_z))\n",
    "        #print((\"grad\",var_Z.mlp.weight.grad))\n",
    "        #print((\"z value\",var_Z.mlp.weight ))\n",
    "        #print((\"mean_value\",torch.mean(var_Z.mlp.weight)))\n",
    "        #optimizer2.step()\n",
    "        optimizer.step()\n",
    "        #print((\"z value_updated\",var_Z.mlp.weight ))\n",
    "       \n",
    "        \n",
    "    model.eval()\n",
    "    val_acc = 0\n",
    "    with torch.inference_mode():\n",
    "        for ind, batch in enumerate(eval_loader):\n",
    "            x, y = input_data\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            test_logits = model(x).squeeze() \n",
    "            test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        #test_loss = loss_fn(test_logits,\n",
    "        #                    y_test)\n",
    "            val_acc += accuracy_fn(y_true=y,\n",
    "                               y_pred=test_pred)\n",
    "    \n",
    "    val_acc /= ind + 1    \n",
    "    if epoch > 2:\n",
    "        print('Epoch %d | Batch %d | train Loss %6.2f, loss1 %6.2f, loss2 % 6.2f, loss3 %6.2f, loss4 %6.2f' % (epoch, batch_num, loss.item(), loss1.item(), loss2.item(), loss3.item(), loss4.item()))\n",
    "        print(' non-zero weights %d | val accuracy %6.2f ' % (sum(torch.nonzero(vec_weight).size()), val_acc) )\n",
    "    print(f\"weight:{vec_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "accuracy = 0\n",
    "for batch_num, input_data in enumerate(train_loader):\n",
    "    x, y = input_data\n",
    "    x = x.to(device).float()\n",
    "    y = y.to(device)\n",
    "    y_pred = model(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
